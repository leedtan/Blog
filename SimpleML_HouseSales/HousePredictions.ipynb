{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import datetime\n",
    "import os\n",
    "from Imputer.knn_imputer import KNNImputer\n",
    "import pdb\n",
    "imagedir = 'images'\n",
    "if not os.path.isdir(imagedir):\n",
    "    os.mkdir(imagedir)\n",
    "\n",
    "# For numeric stability\n",
    "EPSILON = 1e-10\n",
    "\n",
    "df = pd.read_csv(\n",
    "    'houselistings_simulated.csv', parse_dates=['ListingDate', 'SalesDate'])\n",
    "\n",
    "dataset_end = df['SalesDate'].max()\n",
    "\n",
    "FIRST_CUTOFF = pd.to_datetime('2017-01-01')\n",
    "LAST_CUTOFF = df['SalesDate'].max() - pd.DateOffset(months=1)\n",
    "\n",
    "\n",
    "def split_at(df, start_date=None, end_date=None):\n",
    "    data_idx = np.ones(df.shape[0], dtype=bool)\n",
    "    if start_date is not None:\n",
    "        data_idx = data_idx & (df.SalesDate >= start_date)\n",
    "    if end_date is not None:\n",
    "        data_idx = data_idx & (df.ListingDate < end_date)\n",
    "    return df.loc[data_idx, :].copy()\n",
    "\n",
    "\n",
    "def scale(df, scaling_mean=None, scaling_std=None):\n",
    "    numeric_features_train = df.select_dtypes(include=[np.number]).copy()\n",
    "    if scaling_mean is None:\n",
    "        scaling_mean, scaling_std = numeric_features_train.mean(\n",
    "        ), numeric_features_train.std()\n",
    "    numeric_features_train = (\n",
    "        numeric_features_train - scaling_mean) / scaling_std\n",
    "    return numeric_features_train, scaling_mean, scaling_std\n",
    "\n",
    "\n",
    "def build_imputer(numeric_features_train):\n",
    "    imputer = KNNImputer()\n",
    "    imputer.fit(numeric_features_train)\n",
    "    return imputer\n",
    "\n",
    "\n",
    "def apply_imputer(imputer, features):\n",
    "    features = features.copy()\n",
    "    imputer.fill_in(features)\n",
    "    return features\n",
    "\n",
    "\n",
    "def build_Y(df, end):\n",
    "    sold = df['SalesDate'] < end\n",
    "    Y = df['SalesDate'].clip(upper=end).fillna(end) - df['ListingDate']\n",
    "    return sold, Y.dt.days.astype(float)\n",
    "\n",
    "\n",
    "class Model():\n",
    "    def __init__(self, input_size, layer_sizes):\n",
    "        self.input_size = input_size\n",
    "        self.layer_sizes = layer_sizes\n",
    "        smoothing_factor = 1\n",
    "\n",
    "        self.sold = tf.placeholder(tf.float32, shape=(None))\n",
    "        self.x = tf.placeholder(tf.float32, shape=(None, input_size))\n",
    "        self.y = tf.placeholder(tf.float32, shape=(None))\n",
    "\n",
    "        self.layers = [self.x]\n",
    "        for layer_size in layer_sizes:\n",
    "            next_layer = tf.nn.leaky_relu(\n",
    "                tf.layers.dense(self.layers[-1], layer_size))\n",
    "            self.layers.append(next_layer)\n",
    "\n",
    "        self.output = tf.nn.softplus(tf.layers.dense(self.layers[-1], 1))[:, 0]\n",
    "\n",
    "        self.loss_indicator = (tf.cast(self.output < self.y, tf.float32) *\n",
    "                               (1 - self.sold) + self.sold)\n",
    "\n",
    "        def build_loss(loss, loss_indicator):\n",
    "            loss_numerator = tf.reduce_sum(loss * self.loss_indicator)\n",
    "            loss_denominator = (tf.reduce_sum(self.loss_indicator)) + EPSILON\n",
    "            return loss_numerator / loss_denominator\n",
    "\n",
    "        error_by_sample = (self.output - self.y)\n",
    "        percent_error_by_sample = error_by_sample / (self.y + smoothing_factor)\n",
    "\n",
    "        MSPE_raw = tf.reduce_mean(tf.square(percent_error_by_sample))\n",
    "        MSE_raw = tf.reduce_mean(tf.square(error_by_sample))\n",
    "\n",
    "        # Unused, but a demonstration of reasonable loss functions:\n",
    "        self.MSPE = build_loss(MSPE_raw, self.loss_indicator)\n",
    "        self.MSE = build_loss(MSE_raw, self.loss_indicator)\n",
    "\n",
    "        # loss function used that takes into account both raw and percent loss\n",
    "        MSPE_MSE_geometric_mean = tf.sqrt(MSPE_raw * MSE_raw)\n",
    "        self.loss = build_loss(MSE_raw, self.loss_indicator)\n",
    "\n",
    "        MAPE_raw = tf.reduce_mean(tf.abs(percent_error_by_sample))\n",
    "        self.MAPE = build_loss(MAPE_raw, self.loss_indicator)\n",
    "\n",
    "        opt_fcn = tf.train.AdamOptimizer()\n",
    "\n",
    "        def apply_clipped_optimizer(opt_fcn,\n",
    "                                    loss,\n",
    "                                    clip_norm=.1,\n",
    "                                    clip_single=.03,\n",
    "                                    clip_global_norm=False):\n",
    "            gvs = opt_fcn.compute_gradients(loss)\n",
    "\n",
    "            if clip_global_norm:\n",
    "                gs, vs = zip(*[(g, v) for g, v in gvs if g is not None])\n",
    "                capped_gs, grad_norm_total = tf.clip_by_global_norm(\n",
    "                    [g for g in gs], clip_norm)\n",
    "                capped_gvs = list(zip(capped_gs, vs))\n",
    "            else:\n",
    "                grad_norm_total = tf.sqrt(\n",
    "                    tf.reduce_sum([\n",
    "                        tf.reduce_sum(tf.square(grad)) for grad, var in gvs\n",
    "                        if grad is not None\n",
    "                    ]))\n",
    "                capped_gvs = [(tf.clip_by_value(grad, -1 * clip_single,\n",
    "                                                clip_single), var)\n",
    "                              for grad, var in gvs if grad is not None]\n",
    "                capped_gvs = [(tf.clip_by_norm(grad, clip_norm), var)\n",
    "                              for grad, var in capped_gvs if grad is not None]\n",
    "\n",
    "            optimizer = opt_fcn.apply_gradients(\n",
    "                capped_gvs, global_step=tf.train.get_global_step())\n",
    "\n",
    "            return optimizer, grad_norm_total\n",
    "\n",
    "        self.optimizer, self.grad_norm_total = apply_clipped_optimizer(\n",
    "            opt_fcn, self.loss)\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        self.trn_losses = []\n",
    "        self.val_losses = []\n",
    "        self.r2_scores = []\n",
    "\n",
    "    def train_one_epoch(self, X_train, Y_train, sold_train, bs):\n",
    "        # Train an epoch\n",
    "        trn_loss = []\n",
    "        # Randomly shuffle data and prepare for training\n",
    "        trn_samples = X_train.shape[0]\n",
    "        order = np.arange(trn_samples)\n",
    "        np.random.shuffle(order)\n",
    "        num_batches = (trn_samples // bs) + 1\n",
    "        for itr in range(trn_samples // bs):\n",
    "            rows = order[itr * bs:(itr + 1) * bs]\n",
    "            if itr + 1 == num_batches:\n",
    "                rows = order[itr * bs:]\n",
    "            X_active, Y_active, Sold_active = [\n",
    "                mat[rows] for mat in [X_train, Y_train, sold_train]\n",
    "            ]\n",
    "            feed_dict = {\n",
    "                self.x: X_active,\n",
    "                self.y: Y_active,\n",
    "                self.sold: Sold_active\n",
    "            }\n",
    "            _, loss = self.sess.run([self.optimizer, self.loss], feed_dict)\n",
    "            trn_loss.append(loss)\n",
    "        self.trn_losses.append(np.mean(trn_loss))\n",
    "\n",
    "    def validate(self, X_test, Y_test, sold_test, bs=512):\n",
    "        val_samples = X_test.shape[0]\n",
    "        num_batches = val_samples // bs\n",
    "        order = np.arange(val_samples)\n",
    "        val_loss_total = 0\n",
    "        yhat_all = np.zeros(0)\n",
    "        for itr in range(num_batches):\n",
    "            rows = order[itr * bs:(itr + 1) * bs]\n",
    "            if itr + 1 == num_batches:\n",
    "                rows = order[itr * bs:]\n",
    "            X_active, Y_active, Sold_active = [\n",
    "                mat[rows] for mat in [X_test, Y_test, sold_test]\n",
    "            ]\n",
    "            feed_dict = {\n",
    "                self.x: X_active,\n",
    "                self.y: Y_active,\n",
    "                self.sold: Sold_active\n",
    "            }\n",
    "            val_loss, yhat = self.sess.run([self.loss, self.output], feed_dict)\n",
    "            yhat_all = np.concatenate((yhat_all, yhat), 0)\n",
    "            val_loss_total += (val_loss * Sold_active.sum())\n",
    "\n",
    "        val_loss_avg = val_loss_total / sold_test.sum()\n",
    "\n",
    "        self.val_losses.append(val_loss_avg)\n",
    "        self.r2_scores.append(r2_score(Y_test, yhat_all))\n",
    "        return yhat_all\n",
    "\n",
    "    def train(self, Xtrn, Xval, Ytrn, Yval, Soldtrn, Soldval, epochs, bs=512):\n",
    "        # Everything is set. Now train and validate\n",
    "        for epoch in range(epochs):\n",
    "            # run one epoch train and validation\n",
    "            self.train_one_epoch(Xtrn, Ytrn, Soldtrn, bs)\n",
    "            yhat = self.validate(Xval, Yval, Soldval)\n",
    "\n",
    "            if (epoch % 3 == 0) or (epoch == epochs - 1):\n",
    "                # Occasionally print to command line to inspect performance\n",
    "                print('epoch:', epoch, 'train loss: ', self.trn_losses[-1],\n",
    "                      'val loss: ', self.val_losses[-1], 'r2_score:',\n",
    "                      self.r2_scores[-1])\n",
    "        return yhat\n",
    "\n",
    "    def visualize(self, name, fname=None):\n",
    "        # Visualize training and validation losses and r2 scores on one plot\n",
    "        _, ax1 = plt.subplots()\n",
    "        ax2 = ax1.twinx()\n",
    "        ax1.plot(self.trn_losses, label='train loss')\n",
    "        ax1.plot(self.val_losses, label='test loss')\n",
    "        ax2.plot(self.r2_scores, label='validation r2_scores', color='g')\n",
    "        ax1.set_xlabel('epochs')\n",
    "        ax1.set_ylabel('least square losses')\n",
    "        ax2.set_ylabel('R2 Scores')\n",
    "        ax2.legend()\n",
    "        ax1.legend()\n",
    "        plt.title('Losses and r2 scores for ' + name)\n",
    "        if fname is not None:\n",
    "            plt.savefig(imagedir + '/' + fname + '.jpg')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def trn_validate(df, trn_end, val_end, layer_sizes):\n",
    "    df_train = split_at(df, end_date=trn_end)\n",
    "    df_val = split_at(df, start_date=trn_end, end_date=val_end)\n",
    "\n",
    "    numeric_features_train, scaling_mean, scaling_std = scale(df_train)\n",
    "    numeric_features_val, _, _ = scale(df_val, scaling_mean, scaling_std)\n",
    "    nan_trn = numeric_features_train.isnull()\n",
    "    nan_val = numeric_features_val.isnull()\n",
    "    imputer = build_imputer(numeric_features_train)\n",
    "\n",
    "    trn_imputed = apply_imputer(imputer, numeric_features_train)\n",
    "    val_imputed = apply_imputer(imputer, numeric_features_val)\n",
    "\n",
    "    trn_imputed = pd.concat((trn_imputed, nan_trn.astype(float)), 1)\n",
    "    val_imputed = pd.concat((val_imputed, nan_val.astype(float)), 1)\n",
    "\n",
    "    trn_sold, trn_Y = build_Y(df_train, trn_end)\n",
    "\n",
    "    val_sold, val_Y = build_Y(df_val, dataset_end)\n",
    "\n",
    "    n_features = trn_imputed.shape[1]\n",
    "\n",
    "    model = Model(n_features, layer_sizes=layer_sizes)\n",
    "    yhat = model.train(\n",
    "        trn_imputed.values,\n",
    "        val_imputed.values,\n",
    "        trn_Y.values,\n",
    "        val_Y.values,\n",
    "        trn_sold.values,\n",
    "        val_sold.values,\n",
    "        epochs=10)\n",
    "    model.visualize(\n",
    "        'Loss and r2_scores ' + str(trn_end)[:10] + ' to \\n' +\n",
    "        str(val_end)[:10] + ' layers: ' + str(layer_sizes),\n",
    "        str(trn_end)[:7] + ':' + str(val_end)[:7])\n",
    "    return model.val_losses[-1], yhat, val_Y\n",
    "\n",
    "\n",
    "def walkforward_optimization(df, FIRST_CUTOFF, LAST_CUTOFF, months_per_val=1):\n",
    "    trn_end = FIRST_CUTOFF\n",
    "    val_end = FIRST_CUTOFF + pd.DateOffset(months=months_per_val)\n",
    "    validation_losses = pd.DataFrame()\n",
    "    idx = 0\n",
    "    yhat_all, val_Y_all = np.zeros(0), np.zeros(0)\n",
    "    while val_end <= LAST_CUTOFF:\n",
    "        idx += 1\n",
    "        validation_loss, yhat, val_Y = trn_validate(df, trn_end, val_end,\n",
    "                                                    [100, 100])\n",
    "        yhat_all = np.concatenate((yhat_all, yhat.flatten()), 0)\n",
    "        val_Y_all = np.concatenate((val_Y_all, val_Y), 0)\n",
    "        validation_losses.loc[idx, 'cutoffs'] = val_end\n",
    "        validation_losses.loc[idx, 'losses'] = validation_loss\n",
    "        trn_end = trn_end + pd.DateOffset(months=months_per_val)\n",
    "        val_end = val_end + pd.DateOffset(months=months_per_val)\n",
    "    return validation_losses, yhat_all, val_Y_all\n",
    "\n",
    "\n",
    "def visualize_walkforward_optimization_performance(validation_losses):\n",
    "    plt.plot(validation_losses['cutoffs'], validation_losses['losses'])\n",
    "    plt.xticks(rotation=-60)\n",
    "    plt.legend()\n",
    "    plt.xlabel('cutoff time')\n",
    "    plt.ylabel('validation loss')\n",
    "    plt.title('losses throughout walkforward optimization')\n",
    "\n",
    "    plt.savefig(imagedir + '/walkforward.jpg')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_losses, yhat_all, val_Y_all = walkforward_optimization(df, FIRST_CUTOFF, LAST_CUTOFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize_walkforward_optimization_performance(validation_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_by_group(yhat_all, val_Y_all, increment=50, metric='abs'):\n",
    "\n",
    "    # Either plot absolute error by group or directional error by group.\n",
    "    if metric == 'abs':\n",
    "        err = np.abs((yhat_all - val_Y_all) / val_Y_all)\n",
    "        title = \"Absolute Percent Errors by house sales group\"\n",
    "    else:\n",
    "        err = (yhat_all - val_Y_all) / val_Y_all\n",
    "        title = \"Raw Percent Errors by house sales group\"\n",
    "\n",
    "    # group by distances of increment\n",
    "    true_bin = ((val_Y_all // increment) + 1) * increment\n",
    "    err_bins = {}\n",
    "    \n",
    "    # Place errors into appropriate bins\n",
    "    for pe, tb in zip(err, true_bin):\n",
    "        if not tb in err_bins.keys():\n",
    "            err_bins[tb] = []\n",
    "        err_bins[tb] += [pe * 100]\n",
    "\n",
    "    # Sort keys (bins) and organize error lists\n",
    "    err_list = [err_bins[key] for key in sorted(err_bins)]\n",
    "    \n",
    "    #Calculate means and Interquartile ranges\n",
    "    medians = [np.mean(err_list[idx]) for idx in range(len(err_list))]\n",
    "    q25s = [np.percentile(err_list[idx], 25) for idx in range(len(err_list))]\n",
    "    q75s = [np.percentile(err_list[idx], 75) for idx in range(len(err_list))]\n",
    "    \n",
    "    keys = sorted(err_bins)\n",
    "    \n",
    "    # create blank graph and graph the axis variable\n",
    "    _, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "    # label axes\n",
    "    ax.set(\n",
    "        xlabel='true time to sell house',\n",
    "        ylabel='IQR of percent error by group',\n",
    "        title=title)\n",
    "\n",
    "    # Add std deviation bars to the plot\n",
    "    ax.errorbar(\n",
    "        keys,\n",
    "        medians,\n",
    "        yerr=[[medians[i] - q25s[i] for i in range(len(medians))],\n",
    "              [q75s[i] - medians[i] for i in range(len(medians))]],\n",
    "        fmt='-o')\n",
    "    \n",
    "    # set where xticks should occur\n",
    "    ax.set_xticks(keys)\n",
    "    \n",
    "    # Label the x-ticks\n",
    "    xticks = [\n",
    "        str(key - 50)[:-2] + ' to ' + str(key)[:-2] + '\\n' + 'size group:' +\n",
    "        str(len(err_list[idx])) for idx, key in enumerate(sorted(err_bins))\n",
    "    ]\n",
    "    ax.set_xticklabels(xticks)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_by_group(yhat_all, val_Y_all)\n",
    "plot_by_group(yhat_all, val_Y_all, metric='raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
